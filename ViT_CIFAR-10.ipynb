{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x2421f61d390>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# hyperparameters\n",
    "image_size = 32\n",
    "patch_size = 16\n",
    "num_channels = 3\n",
    "num_classes = 10\n",
    "d_model = 384\n",
    "num_heads = 12\n",
    "num_layers = 7\n",
    "learning_rate = 2e-3\n",
    "batch_size = 64\n",
    "num_epochs = 20\n",
    "dropout = 0\n",
    "\n",
    "# device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# seed\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data transforms (image augmentations)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(image_size),\n",
    "    transforms.RandomHorizontalFlip(p=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "no_transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "denormalize = transforms.Normalize(\n",
    "    mean=[-0.485/0.229, -0.456/0.224, -0.406/0.225],\n",
    "    std=[1/0.229, 1/0.224, 1/0.225]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Load the CIFAR-10 dataset\n",
    "#torchvision.datasets.CIFAR10.url=\"http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\" # fix URL error\n",
    "\n",
    "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, transform=transform, download=True)\n",
    "valid_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, transform=no_transform, download=True)\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size, shuffle=True)\n",
    "valid_dataloader = torch.utils.data.DataLoader(valid_dataset, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "image, label = next(iter(valid_dataloader))\n",
    "\n",
    "assert image_size == image.shape[-1]\n",
    "assert num_channels == image.shape[-3]\n",
    "\n",
    "plt.imshow(denormalize(image)[0].permute(1,2,0))\n",
    "print(label[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_patches = (image_size // patch_size) ** 2\n",
    "patch_embedding = nn.Conv2d(in_channels=1, out_channels=d_model, kernel_size=patch_size, stride=patch_size)\n",
    "cls_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "pos_embed = nn.Parameter(torch.randn(1, 1 + num_patches, d_model))\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=num_heads, dim_feedforward=d_model * 4, dropout=0.1)\n",
    "encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "classification_head = nn.Linear(d_model, num_classes)\n",
    "\n",
    "x = patch_embedding(image) # (B, C, H, W)\n",
    "x = x.flatten(2).transpose(1, 2) # (B, T, C), T=H*W\n",
    "(B, T, C) = x.shape\n",
    "print(x.shape)\n",
    "\n",
    "cls_tokens = cls_token.repeat(B, 1, 1) # (1, 1, C) -> (B, 1, C)\n",
    "x = torch.cat([cls_tokens, x], dim=1) # (B, T+1, C)\n",
    "print(str(cls_token.shape) + ' ->', str(cls_tokens.shape))\n",
    "print(x.shape)\n",
    "\n",
    "x = x + pos_embed[:, : T + 1] # (B, T+1, C)\n",
    "print(pos_embed.shape, pos_embed[:, : T + 1].shape)\n",
    "print(x.shape)\n",
    "\n",
    "x = encoder(x)\n",
    "print(x.shape)\n",
    "x = x.mean(dim=1)\n",
    "print(x.shape)\n",
    "classification_head(x)\n",
    "print(x.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO act, conv bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    def __init__(self, image_size, patch_size, num_channels, num_classes, embed_dim, num_heads, num_layers, is_cls_tok=False, dropout=0.5):\n",
    "        super().__init__()\n",
    "        self.is_cls_tok = is_cls_tok\n",
    "\n",
    "        # Compute the number of patches\n",
    "        self.num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        # Patch Embedding\n",
    "        self.patch_embed = nn.Conv2d(in_channels=num_channels, out_channels=embed_dim, kernel_size=patch_size, stride=patch_size) # bias=False\n",
    "\n",
    "        # Positional encoding\n",
    "        if is_cls_tok:\n",
    "            self.cls_tok = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "            self.pos_emb = nn.Parameter(torch.randn(1, 1 + self.num_patches, embed_dim))\n",
    "        else:\n",
    "            self.cls_tok = None\n",
    "            self.pos_emb = nn.Parameter(torch.randn(1, self.num_patches, embed_dim))\n",
    "\n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=embed_dim*4, dropout=0.1)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "\n",
    "        # Dropout\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        # Layer Norm\n",
    "        self.ln = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        # Classification Head\n",
    "        self.classification_head = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # Patch Embedding\n",
    "        x = self.patch_embed(x) # (B, C, H, W)\n",
    "\n",
    "        # reshape Patches into sequence\n",
    "        x = x.flatten(2).transpose(1, 2) # (B, T, C), T=H*W\n",
    "        B, T, C = x.shape\n",
    "\n",
    "        # add Class Token\n",
    "        if self.is_cls_tok:\n",
    "            cls_tokens = self.cls_tok.repeat(B, 1, 1) # (1, 1, C) -> (B, 1, C)\n",
    "            x = torch.cat([cls_tokens, x], dim=1)\n",
    "\n",
    "        # add Positional Embedding\n",
    "        x = x + self.pos_emb # [:, : T + 1, :]\n",
    "\n",
    "        # Dropout\n",
    "        x = self.dropout(x)\n",
    "\n",
    "        # Transformer Encoder\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Classification Head\n",
    "        if self.is_cls_tok:\n",
    "            x = x[:,0]\n",
    "        else:\n",
    "            x = x.mean(dim=1)  # global average pooling\n",
    "        x = self.ln(x)\n",
    "        x = self.classification_head(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of parameters: 12722698\n"
     ]
    }
   ],
   "source": [
    "# define model\n",
    "model = ViT(image_size, patch_size, num_channels, num_classes, d_model, num_heads, num_layers, is_cls_tok=False, dropout=dropout).to(device)\n",
    "print('# of parameters:', sum(p.numel() for p in model.parameters() if p.requires_grad))\n",
    "# model = torch.compile(model) # requires pytorch >= 2.0\n",
    "\n",
    "# optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1668,  0.1653, -0.1987, -0.0517, -0.3182, -0.4312,  0.4526,  0.4717,\n",
       "         0.0242,  0.2208], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.randn(64,3,32,32))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, dataloader):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # accumulate loss across batches\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        #x, y = batch[0], batch[1] # move to device\n",
    "\n",
    "        optimizer.zero_grad() # reset gradients\n",
    "        y_pred = model(x) # forward pass\n",
    "        loss = criterion(y_pred, y) # calculate loss\n",
    "        loss.backward() # compute gradients\n",
    "        optimizer.step() # optimization step\n",
    "\n",
    "        running_loss += loss\n",
    "        running_accuracy += (y_pred.argmax(dim=1) == y).float().mean()\n",
    "\n",
    "    # average loss across batches\n",
    "    loss = running_loss / len(dataloader)\n",
    "    accuracy = running_accuracy / len(dataloader)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def valid(model, dataloader):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # accumulate loss across batches\n",
    "    running_loss = 0.0\n",
    "    running_accuracy = 0.0\n",
    "\n",
    "    for x, y in dataloader:\n",
    "        #x, y = x.to(device), y.to(device) # move to device\n",
    "\n",
    "        y_pred = model(x) # forward pass\n",
    "        loss = criterion(y_pred, y) # calculate loss\n",
    "\n",
    "        running_loss += loss\n",
    "        running_accuracy += (y_pred.argmax(dim=1) == y).float().mean()\n",
    "    \n",
    "    # average loss across batches\n",
    "    loss = running_loss / len(dataloader)\n",
    "    accuracy = running_accuracy / len(dataloader)\n",
    "\n",
    "    return loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0\n",
      "training\n",
      "loss: 2.3375003337860107, accuracy: 0.09938459098339081\n",
      "validating\n",
      "val_loss: 2.30583119392395, val_accuracy: 0.09996403753757477\n",
      "epoch 1\n",
      "training\n",
      "loss: 2.305652141571045, accuracy: 0.09840553253889084\n",
      "validating\n",
      "val_loss: 2.303351640701294, val_accuracy: 0.10002397745847702\n",
      "epoch 2\n",
      "training\n",
      "loss: 2.303988456726074, accuracy: 0.09954443573951721\n",
      "validating\n",
      "val_loss: 2.3028640747070312, val_accuracy: 0.09996403753757477\n",
      "epoch 3\n",
      "training\n",
      "loss: 2.303583860397339, accuracy: 0.10070332139730453\n",
      "validating\n",
      "val_loss: 2.3033993244171143, val_accuracy: 0.09996403753757477\n",
      "epoch 4\n",
      "training\n",
      "loss: 2.3036396503448486, accuracy: 0.09824568778276443\n",
      "validating\n",
      "val_loss: 2.3028671741485596, val_accuracy: 0.09990409016609192\n",
      "epoch 5\n",
      "training\n",
      "loss: 2.303689956665039, accuracy: 0.09852541238069534\n",
      "validating\n",
      "val_loss: 2.3034377098083496, val_accuracy: 0.10020380467176437\n",
      "epoch 6\n",
      "training\n",
      "loss: 2.303866147994995, accuracy: 0.09792599081993103\n",
      "validating\n",
      "val_loss: 2.3030459880828857, val_accuracy: 0.10002397745847702\n",
      "epoch 7\n",
      "training\n",
      "loss: 2.303630828857422, accuracy: 0.09834558516740799\n",
      "validating\n",
      "val_loss: 2.3033974170684814, val_accuracy: 0.09996403753757477\n",
      "epoch 8\n",
      "training\n",
      "loss: 2.303828239440918, accuracy: 0.09876518696546555\n",
      "validating\n",
      "val_loss: 2.3030312061309814, val_accuracy: 0.09996403753757477\n",
      "epoch 9\n",
      "training\n",
      "loss: 2.3037407398223877, accuracy: 0.10026374459266663\n",
      "validating\n",
      "val_loss: 2.3029723167419434, val_accuracy: 0.10014386475086212\n",
      "epoch 10\n",
      "training\n",
      "loss: 2.30354380607605, accuracy: 0.09832561016082764\n",
      "validating\n",
      "val_loss: 2.3034496307373047, val_accuracy: 0.09996403753757477\n",
      "epoch 11\n",
      "training\n",
      "loss: 2.303659677505493, accuracy: 0.0960477963089943\n",
      "validating\n",
      "val_loss: 2.3030242919921875, val_accuracy: 0.10002397745847702\n",
      "epoch 12\n",
      "training\n",
      "loss: 2.303511619567871, accuracy: 0.10104300081729889\n",
      "validating\n",
      "val_loss: 2.302711009979248, val_accuracy: 0.10002397745847702\n",
      "epoch 13\n",
      "training\n",
      "loss: 2.3034822940826416, accuracy: 0.09936460852622986\n",
      "validating\n",
      "val_loss: 2.303041696548462, val_accuracy: 0.10002397745847702\n",
      "epoch 14\n",
      "training\n",
      "loss: 2.303229808807373, accuracy: 0.1016823872923851\n",
      "validating\n",
      "val_loss: 2.303284168243408, val_accuracy: 0.10002397745847702\n",
      "epoch 15\n",
      "training\n",
      "loss: 2.3032984733581543, accuracy: 0.10040361434221268\n",
      "validating\n",
      "val_loss: 2.3028762340545654, val_accuracy: 0.09996403753757477\n",
      "epoch 16\n",
      "training\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mepoch \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m loss, accuracy \u001b[39m=\u001b[39m train(model, train_dataloader)\n\u001b[0;32m     11\u001b[0m train_losses\u001b[39m.\u001b[39mappend(loss)\n\u001b[0;32m     12\u001b[0m train_accs\u001b[39m.\u001b[39mappend(accuracy)\n",
      "Cell \u001b[1;32mIn[9], line 15\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, dataloader)\u001b[0m\n\u001b[0;32m     13\u001b[0m y_pred \u001b[39m=\u001b[39m model(x) \u001b[39m# forward pass\u001b[39;00m\n\u001b[0;32m     14\u001b[0m loss \u001b[39m=\u001b[39m criterion(y_pred, y) \u001b[39m# calculate loss\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m loss\u001b[39m.\u001b[39;49mbackward() \u001b[39m# compute gradients\u001b[39;00m\n\u001b[0;32m     16\u001b[0m optimizer\u001b[39m.\u001b[39mstep() \u001b[39m# optimization step\u001b[39;00m\n\u001b[0;32m     18\u001b[0m running_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n",
      "File \u001b[1;32mc:\\Users\\ikang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    479\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    480\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    481\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    486\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[0;32m    487\u001b[0m     )\n\u001b[1;32m--> 488\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[0;32m    489\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[0;32m    490\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\ikang\\anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    192\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m    194\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    195\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    196\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 197\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    198\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    199\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# keeping track of losses as it happen\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accs = []\n",
    "valid_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'epoch {epoch}')\n",
    "    print('training')\n",
    "    loss, accuracy = train(model, train_dataloader)\n",
    "    train_losses.append(loss)\n",
    "    train_accs.append(accuracy)\n",
    "    print(f'loss: {loss}, accuracy: {accuracy}')\n",
    "    \n",
    "    print('validating')\n",
    "    loss, accuracy = valid(model, train_dataloader)\n",
    "    valid_losses.append(loss)\n",
    "    valid_accs.append(accuracy)\n",
    "    print(f'val_loss: {loss}, val_accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
